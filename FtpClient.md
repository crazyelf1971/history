# FTP клиент - очень продвинутый, почти искусственный интеллект

Суть задачи была в том, что нужно было забирать с ftp сервера файлы, которые туда массово выкладывались. Казалось бы банальная вещь, но в процессе разработки пришлось столкнуться с очень интересными вызовами.


| Проблема | Решение |
| ------- | -------- |
| Файлов много, скорость скачивания недостаточна | Это было самое простое: я сделал несколько потоков, разгребающих очередь, и сделал пул ftp коннекшенов. Почему эти сущности пришлось разделить. Ftp поддерживал не более скольки-то коннекшенов, но при этом выгодно было делать больше потоков обработки, чем коннектов к ftp - потоки делали сопутствующую обработку файлов (проверка целостности), работали с очередью и т.д., так процессорное время лучше утилизовалось. Про парадигму `async/await` я тогда ещё не знал, а то бы, конечно, лучше сделал через неё. |
| Большие файлы не могут докачаться | Параллельная скачка прекрасна, но интернет не всегда стабилен, плюс к тому параллельные коннекты к серверу тормозят скачивание, а то и могут вообще уронить ftp server в каких-то случаях, как дальше выяснилось. В результате вероятность того, что загрузка большого файла прервётся по тем или иным причинам, была довольно велика. А докачка файлов сервером не поддерживалась. Пришлось добавлять настройку "максимальный размер файла для параллельной скачки" и заводить отдельную очередь для больших файлов. Сначала параллельно качались мелкие файлы, очередь скачки шустро разгребалась, а в конце процесса отдельно обрабатывалась в один поток очередь больших файлов, так, чтобы их скачке ничего не мешало. |
| Обращение к некоторым файлам/каталогам роняет сервер | Можно было бы вообще обойтись без очередей и просто как-то рекурсивно обходить каталоги и качать файлы. Но могла прерваться связь, могли попасться файлы, при обращении к которым ftp сервер перезапускался. Поэтому пришлось делать полноценную очередь задач на сбор содержимого каталогов и скачку файлов. Для проблемных файлов была предназначена отдельная, вторая очередь, в которую перемещались проблемные файлы. Возможен и другой вариант, более универсальный - со счётчиком попыток, при увеличении счётчика после неудачной попытки ставится пауза - задача не может запуститься ранее, чем через какой-то промежуток времени, причём, чем больше номер попытки, тем больше эта пауза. Но в данной задаче достаточно оказалось просто отдельной очереди для второй попытки. |
| Слишком много файлов в одном каталоге, обращение к серверу сильно тормозит | Есть известная проблема, что когда в одном каталоге (без учёта подкаталогов) находится больше какого-то кол-ва файлов (2000 или 20000, не суть в общем-то важно), некоторые файловые системы начинают очень сильно тормозить при попытке прочитать содержимое такого каталога. Причём, задержка не прямо пропорциональна кол-ву файлов, а растёт быстрее, экспоненциально. В какой-то момент это стало серьёзной проблемой. Помогло тут то, что имена файлов на этом сервере формировались в виде `GUID`-ов, т.е. состояли довольно равномерно из символов диапазона `0-9A-F` в каких-то сочетаниях. Я вместо запроса всех файлов в каталоге стал "засылать пробу" (кто играл в космические стратегии, тому знаком этот термин). Т.е. запрашивал для начала в каталоге только файлы, начинающиеся на случайный символ из этого диапазона. Если сервер выдавал в ответ менее определённого числа имён файлов (допустим, 100 - для этого была сделана дополнительная настройка программы), то дальше можно было спокойно запрашивать всё содержимое каталога. А если файлов было больше, то запрашивались в случайном порядке файлы на остальные буквы, а во второй очереди - ещё и на \*, вдруг в каталоге окажутся какие-то неожиданные имена, не охваченные предыдущими шаблонами. Случайность выбора символов была важна из-за потенциального наличия файлов, роняющих сервер. Важно было максимально перемешивать очередь запросов на всех этапах, чтобы так или иначе в этот или в следующий раз все файлы смогли скачаться. |
